"use client";

import { useCallback, useEffect, useRef, useState } from "react";
import Video, { Room, LocalVideoTrack, RemoteVideoTrack, createLocalVideoTrack } from "twilio-video";
import { Client as ConversationsClient, Conversation, Message as ConversationMessage } from "@twilio/conversations";
import MeetingLayout from "@/components/layouts/meeting/meeting-layout";
import UserCard from "@/components/ui/user-card";
import { Button } from "@/components/ui/button";
import { Input } from "@/components/ui/input";
import { useTheme } from "@/contexts/ThemeContext";
import { cn } from "@/utils/cn";
import { VideoIcon, VideoOffIcon, PhoneIcon, Mic as MicrophoneIcon, MicOff as MicrophoneOffIcon, MessageSquare, Send, ArrowRight } from "lucide-react";
import OpenAI from "openai";

// Định nghĩa types cho Web Speech API
interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
  resultIndex: number;
}

interface SpeechRecognition extends EventTarget {
  continuous: boolean;
  interimResults: boolean;
  lang: string;
  start: () => void;
  stop: () => void;
  abort: () => void;
  onerror: (event: Event) => void;
  onresult: (event: SpeechRecognitionEvent) => void;
  onstart: (event: Event) => void;
  onend: (event: Event) => void;
}

// Đảm bảo SpeechRecognition có sẵn trên window
declare global {
  interface Window {
    SpeechRecognition: new () => SpeechRecognition;
    webkitSpeechRecognition: new () => SpeechRecognition;
  }
}

const WEBSOCKET_URL = "wss://asl-sign-language-336987311239.us-central1.run.app/ws";

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.NEXT_PUBLIC_OPENAI_API_KEY,
  dangerouslyAllowBrowser: true
});

// Log API key status for debugging (without revealing the key)

// Interface for prediction queue items
interface PredictionItem {
  prediction: string;
  confidence: number;
  timestamp: number;
  processed: boolean;
}

// Interface for chat messages
interface ChatMessage {
  sender: string;
  content: string;
  timestamp: number;
  isLocal: boolean;
}

// Add a function to get a token from the Auth service
const getConversationsToken = async (identity: string) => {
  try {
    const response = await fetch('https://chattryoutautogeneratedauthservice-4161.twil.io/login', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        identity: identity, // Use User1, User2, or User3 as identity
        // No password needed for this service
      }),
    });

    if (!response.ok) {
      throw new Error(`Failed to fetch token: ${response.status}`);
    }

    const token = await response.text();
    return token;
  } catch (error) {
    console.error('Error fetching Conversations token:', error);
    throw error;
  }
};

export default function Meeting() {
  const theme = useTheme();
  
  let isDarkMode = false;
  try {
    isDarkMode = theme?.isDarkMode ?? false;
  } catch (error) {
    console.error("Error getting theme context:", error);
  }
  
  const [room, setRoom] = useState<Room | null>(null);
  const [status, setStatus] = useState("Chưa kết nối");
  const [hasMediaPermission, setHasMediaPermission] = useState(false);
  const [userName, setUserName] = useState("");
  const [isJoining, setIsJoining] = useState(false);
  const [localVideoTrack, setLocalVideoTrack] = useState<LocalVideoTrack | null>(null);
  const [previewTrack, setPreviewTrack] = useState<LocalVideoTrack | null>(null);
  const [remoteParticipants, setRemoteParticipants] = useState<
    {
      id: string;
      username: string;
      track: RemoteVideoTrack;
      isMicOn: boolean;
    }[]
  >([]);
  const [isMicOn, setIsMicOn] = useState(true);
  const [isVideoOn, setIsVideoOn] = useState(true);
  const previewRef = useRef<HTMLVideoElement>(null);
  const previewStarted = useRef(false);
  
  // For debugging - keep a ref copy of prediction queue that doesn't rely on React state
  const currentPredictionsRef = useRef<PredictionItem[]>([]);
  
  // Chat functionality
  const [chatMessages, setChatMessages] = useState<ChatMessage[]>([]);
  const [newMessage, setNewMessage] = useState<string>("");
  const [isChatOpen, setIsChatOpen] = useState<boolean>(false);
  const chatEndRef = useRef<HTMLDivElement>(null);
  const [conversationsClient, setConversationsClient] = useState<ConversationsClient | null>(null);
  const [conversation, setConversation] = useState<Conversation | null>(null);
  
  // Speech-to-text functionality
  const [isListening, setIsListening] = useState<boolean>(false);
  const [speechText, setSpeechText] = useState<string>("");
  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const speechIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const lastSentSpeechRef = useRef<string>("");
  
  // Sign language recognition WebSocket states
  const [socket, setSocket] = useState<WebSocket | null>(null);
  const [socketConnected, setSocketConnected] = useState(false);
  const [signPrediction, setSignPrediction] = useState("No sign detected");
  const [signConfidence, setSignConfidence] = useState(0);
  const [isRecognitionActive, setIsRecognitionActive] = useState(false);
  const captureIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const videoElementRef = useRef<HTMLVideoElement | null>(null);
  
  // Prediction queue with timestamps
  const [predictionQueue, setPredictionQueue] = useState<PredictionItem[]>([]);
  const [processedSubtitles, setProcessedSubtitles] = useState<string>("");
  const [combinedSubtitles, setCombinedSubtitles] = useState<string>("");
  const processingIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const isProcessingRef = useRef<boolean>(false);

  // Define captureAndSendFrame function before it's used in other functions
  const captureAndSendFrame = useCallback(() => {
    const videoElement = videoElementRef.current;
    if (!videoElement || !socket || socket.readyState !== WebSocket.OPEN) return;
    
    // Create canvas to capture frame
    const canvas = document.createElement("canvas");
    canvas.width = videoElement.videoWidth;
    canvas.height = videoElement.videoHeight;
    
    const ctx = canvas.getContext("2d");
    if (ctx) {
      ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
      
      // Convert to base64 JPEG with reduced quality for performance
      const imageData = canvas.toDataURL("image/jpeg", 0.7);
      
      // Send data to WebSocket
      socket.send(imageData);
    }
  }, [socket]);

  // Optimize preview camera by creating track once
  const startVideoPreview = async () => {
    // Prevent multiple preview attempts
    if (previewStarted.current) return;
    previewStarted.current = true;
    
    try {
      // Create local video track directly using Twilio's method
      const track = await createLocalVideoTrack({
        width: 640,
        height: 480,
        frameRate: 24 // Lower frame rate for smoother preview
      });
      
      setPreviewTrack(track);
      setHasMediaPermission(true);
      
      // Check if audio is also accessible
      await navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          stream.getTracks().forEach(t => t.stop());
        });
      
      return true;
    } catch (err) {
      console.error("Camera preview error:", err);
      setStatus("Không thể truy cập camera/microphone. Vui lòng kiểm tra quyền.");
      setHasMediaPermission(false);
      previewStarted.current = false;
      return false;
    }
  };

  // Attach preview track to video element
  useEffect(() => {
    if (previewTrack && previewRef.current) {
      // Clean up any existing attachments first to prevent duplicate
      const existingElements = previewTrack.detach();
      existingElements.forEach(element => element.remove());
      
      // Attach to our ref
      previewTrack.attach(previewRef.current);
    }
    
    return () => {
      if (previewTrack) {
        previewTrack.detach();
      }
    };
  }, [previewTrack]);

  // Start preview when component mounts
  useEffect(() => {
    startVideoPreview();
    return () => {
      if (previewTrack) {
        previewTrack.detach();
        previewTrack.stop();
        previewStarted.current = false;
      }
    };
  }, [previewTrack]);

  // Kiểm tra quyền media - now just checks permission without creating preview
  const checkMediaPermissions = useCallback(async () => {
    if (hasMediaPermission) return true;
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: true,
        audio: true,
      });
      stream.getTracks().forEach((t) => t.stop());
      setHasMediaPermission(true);
      return true;
    } catch {
      setStatus("Không thể truy cập camera/microphone. Vui lòng kiểm tra quyền.");
      setHasMediaPermission(false);
      return false;
    }
  }, [hasMediaPermission, setStatus]);

  // Connect to WebSocket for sign language recognition
  const connectWebSocket = useCallback(() => {
    if (socket) {
      socket.close();
    }

    try {
      console.log("Connecting to WebSocket...");
      const newSocket = new WebSocket(WEBSOCKET_URL);

      newSocket.onopen = () => {
        console.log("WebSocket connection established");
        setSocketConnected(true);
      };

      newSocket.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          // Remove verbose logging
          // console.log("Received sign recognition data:", data);
          
          let prediction = "";
          let confidence = 0;
          
          // Update prediction and confidence
          if (data.prediction) {
            prediction = data.prediction;
            confidence = Math.round(data.confidence * 100);
          } else if (data.current_word) {
            // Alternative format where the server might send a 'word' property
            prediction = data.current_word;
            confidence = data.confidence ? Math.round(data.confidence * 100) : 0;
          } else {
            prediction = "No sign detected";
            confidence = 0;
          }
          
          // Update UI for immediate feedback
          setSignPrediction(prediction);
          setSignConfidence(confidence);
          
          // Add to prediction queue
          handleNewPrediction(prediction, confidence);
        } catch (error) {
          console.error("Error parsing WebSocket message:", error);
        }
      };

      newSocket.onclose = () => {
        console.log("WebSocket connection closed");
        setSocketConnected(false);
        setIsRecognitionActive(false);
        if (captureIntervalRef.current) {
          clearInterval(captureIntervalRef.current);
          captureIntervalRef.current = null;
        }
      };

      newSocket.onerror = (error) => {
        console.error("WebSocket error:", error);
      };

      setSocket(newSocket);
    } catch (err) {
      console.error("Error connecting to WebSocket:", err);
    }
  }, [socket]);

  // Capture and send video frame to WebSocket
  useEffect(() => {
    // Skip if not connected or video element not available
    if (!socketConnected || !socket || socket.readyState !== WebSocket.OPEN || !videoElementRef.current) {
      return;
    }

    // Set up interval for frame capture
    const intervalId = setInterval(() => {
      const videoElement = videoElementRef.current;
      if (!videoElement) return;
      
      // Create canvas to capture frame
      const canvas = document.createElement("canvas");
      canvas.width = videoElement.videoWidth;
      canvas.height = videoElement.videoHeight;
      
      const ctx = canvas.getContext("2d");
      if (ctx) {
        ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
        
        // Convert to base64 JPEG with reduced quality for performance
        const imageData = canvas.toDataURL("image/jpeg", 0.7);
        
        // Send data directly without JSON wrapping
        socket.send(imageData);
      }
    }, 100); // Capture at 10fps

    // Clean up interval on component unmount or when dependencies change
    return () => {
      clearInterval(intervalId);
    };
  }, [socket, socketConnected]);

  // Process queue items using OpenAI
  const processQueue = useCallback(async () => {
    // Prevent concurrent processing
    if (isProcessingRef.current) return;
    isProcessingRef.current = true;
    
    try {
      console.log("Starting to process predictions with OpenAI");
      
      // Get all unprocessed items from the last 30 seconds
      const currentTime = Date.now();
      const thirtySecondsAgo = currentTime - 30 * 1000; // 30 seconds
      
      // Debug current time and cutoff time
      console.log("Current time:", new Date(currentTime).toISOString());
      console.log("Cutoff time (30s ago):", new Date(thirtySecondsAgo).toISOString());
      
      // Debug current queue state
      console.log("Total queue items:", predictionQueue.length);
      console.log("Items with timestamps:", predictionQueue.map(item => ({
        prediction: item.prediction,
        timestamp: new Date(item.timestamp).toISOString(),
        processed: item.processed
      })));
      
      // Debug ref data too
      console.log("Items in ref:", currentPredictionsRef.current.length);
      
      // If state is empty but ref has data, use ref data
      let queueToUse = predictionQueue;
      if (predictionQueue.length === 0 && currentPredictionsRef.current.length > 0) {
        console.log("State queue is empty but ref has data, using ref data");
        queueToUse = currentPredictionsRef.current;
      }
      
      // Option 1: Filter items from the last 30 seconds that haven't been processed
      const recentItemsToProcess = queueToUse.filter(
        item => item.timestamp >= thirtySecondsAgo && !item.processed
      );
      
      // Option 2: Get all unprocessed items if recent filtering yields nothing
      let itemsToProcess = recentItemsToProcess;
      if (recentItemsToProcess.length === 0 && predictionQueue.some(item => !item.processed)) {
        console.log("No recent items, using all unprocessed items instead");
        itemsToProcess = predictionQueue.filter(item => !item.processed);
      }
      
      // Debug filtered items
      console.log("Items to process after filtering:", itemsToProcess.length);
      
      if (itemsToProcess.length === 0) {
        console.log("No items to process, skipping OpenAI call");
        isProcessingRef.current = false;
        return;
      }
      
      console.log(`Processing ${itemsToProcess.length} items from the last 30 seconds`);
      
      // Extract raw predictions
      const rawPredictions = itemsToProcess.map(item => item.prediction).join(", ");
      
      // Skip processing if there's nothing meaningful to process
      if (!rawPredictions || rawPredictions.trim() === "") {
        console.log("Empty raw predictions, skipping OpenAI call");
        isProcessingRef.current = false;
        return;
      }
      
      // Create a prompt for the LLM
      const prompt = `
You are a sign language interpreter assistant. Below is a series of detected signs from the past 30 seconds that may contain:
1. Repeated words (e.g., "hello, hello, hello")
2. Noise or irrelevant detections
3. Missing words that would make the sentence more coherent

Raw detected signs from the last 30 seconds: "${rawPredictions}"

Your task:
1. Remove unnecessary repetitions
2. Fill in any missing words to make the sentence grammatically correct and meaningful
3. Format it as a proper, coherent subtitle
4. Keep it concise and natural
5. If there are multiple unrelated phrases, format them as a coherent paragraph

Return ONLY the cleaned-up subtitle text, nothing else.
`;
      
      // Call OpenAI API
      console.log(`[${new Date().toISOString()}] Calling OpenAI with prompt:`, prompt);
      
      const response = await openai.chat.completions.create({
        model: "gpt-4o-mini",
        messages: [
          { role: "system", content: "You are a sign language interpreter assistant." },
          { role: "user", content: prompt }
        ],
      });
      
      console.log(`[${new Date().toISOString()}] OpenAI Response:`, JSON.stringify(response, null, 2));
      
      const processedText = response.choices[0].message.content || "";
      console.log(`[${new Date().toISOString()}] Processed text:`, processedText);
      
      // Update processed subtitles
      setProcessedSubtitles(processedText);
      
      // Mark items as processed
      if (itemsToProcess.length > 0) {
        console.log("Marking items as processed");
        setPredictionQueue(prevQueue => 
          prevQueue.map(item => {
            // Find if this item is in the items to process list
            const shouldBeProcessed = itemsToProcess.some(
              processItem => processItem.timestamp === item.timestamp && 
                             processItem.prediction === item.prediction
            );
            
            // Only mark as processed if it was in our processing list
            if (shouldBeProcessed) {
              console.log(`Marking item as processed: ${item.prediction}`);
              return { ...item, processed: true };
            }
            return item;
          })
        );
      }
    } catch (error) {
      console.error("Error processing predictions:", error);
    } finally {
      isProcessingRef.current = false;
    }
  }, [predictionQueue]);

  // Toggle sign language recognition
  const toggleSignRecognition = useCallback(() => {
    if (isRecognitionActive) {
      console.log("Stopping sign recognition and processing");
      // Stop recognition
      if (captureIntervalRef.current) {
        clearInterval(captureIntervalRef.current);
        captureIntervalRef.current = null;
      }
      
      // Stop processing interval
      if (processingIntervalRef.current) {
        clearInterval(processingIntervalRef.current);
        processingIntervalRef.current = null;
      }
      
      setIsRecognitionActive(false);
      
      // Clear current prediction
      setSignPrediction("No sign detected");
      setSignConfidence(0);
      setProcessedSubtitles("");
      setCombinedSubtitles("");
      
      // Clear prediction queue
      setPredictionQueue([]);
      currentPredictionsRef.current = [];
      console.log("Cleared prediction queue and ref");
    } else {
      console.log("Starting sign recognition and processing");
      // Start recognition if connected
      if (socketConnected && videoElementRef.current) {
        // Reset states
        setIsRecognitionActive(true);
        setPredictionQueue([]);
        currentPredictionsRef.current = [];
        setProcessedSubtitles("");
        setCombinedSubtitles("");
        console.log("Reset prediction queue and subtitles");
        
        // Capture frames at 10 FPS (100ms interval) - matching reference code
        captureIntervalRef.current = setInterval(captureAndSendFrame, 100);
        
        // Process queue every 30 seconds for more frequent updates
        console.log("Setting up processing interval every 30 seconds");
        processingIntervalRef.current = setInterval(() => {
          console.log("Processing interval triggered");
          processQueue();
        }, 30000); // Changed to 30 seconds
        
        // Force an immediate processing after 5 seconds to test
        setTimeout(() => {
          console.log("Forcing initial processing");
          processQueue();
        }, 5000);
      } else if (!socketConnected) {
        // Try to connect first
        connectWebSocket();
      }
    }
  }, [isRecognitionActive, socketConnected, socket, captureAndSendFrame, connectWebSocket, processQueue]);

  // Define setupConversationEvents with proper type handling
  const setupConversationEvents = useCallback(async (conv: Conversation) => {
    try {
      // Load existing messages
      const messages = await conv.getMessages();
      const formattedMessages = messages.items.map((msg: ConversationMessage): ChatMessage => ({
        sender: msg.author || "Unknown", // Handle null author
        content: msg.body || "",
        timestamp: msg.dateCreated?.getTime() || Date.now(), // Handle null date
        isLocal: (msg.author || "") === userName
      }));
      
      setChatMessages(formattedMessages);
      
      // Listen for new messages
      conv.on('messageAdded', (msg: ConversationMessage) => {
        console.log('New message added:', msg.body);
        const newMessage: ChatMessage = {
          sender: msg.author || "Unknown", // Handle null author
          content: msg.body || "",
          timestamp: msg.dateCreated?.getTime() || Date.now(), // Handle null date
          isLocal: (msg.author || "") === userName
        };
        
        setChatMessages(prev => [...prev, newMessage]);
        
        // Scroll to bottom
        setTimeout(() => {
          chatEndRef.current?.scrollIntoView({ behavior: "smooth" });
        }, 100);
      });
    } catch (error: any) {
      console.error("Error setting up conversation events:", error);
    }
  }, [userName]);

  // Update the Conversations client initialization to use the proper event-based pattern
  const initializeConversations = useCallback(async (token: string) => {
    try {
      console.log("Initializing Twilio Conversations...");
      const client = new ConversationsClient(token);
      
      // Store client reference immediately
      setConversationsClient(client);
      
      // Set up event listeners
      client.on("connectionStateChanged", (state: string) => {
        console.log("Client connection state changed to:", state);
      });
      
      client.on("stateChanged", (state: string) => {
        console.log("Client state changed to:", state);
      });
      
      // Set up initialized handler
      client.on("initialized", async () => {
        console.log("Conversations client initialized successfully");
        
        // Try to join the conversation for this meeting room
        const conversationName = "MeetingRoom";
        
        try {
          console.log("Attempting to join conversation:", conversationName);
          const conv = await client.getConversationByUniqueName(conversationName);
          setConversation(conv);
          await setupConversationEvents(conv);
        } catch (error: any) {
          console.log("Conversation not found, creating a new one");
          try {
            // Create a new conversation if it doesn't exist
            const newConv = await client.createConversation({
              uniqueName: conversationName,
              friendlyName: "Meeting Room Chat"
            });
            
            setConversation(newConv);
            await newConv.join();
            await setupConversationEvents(newConv);
          } catch (createError: any) {
            console.error("Error creating conversation:", createError);
          }
        }
      });
      
      // Handle initialization failure - fix type to make error optional
      client.on("initFailed", ({ error }: { error?: any }) => {
        console.error("Failed to initialize the conversations client:", error);
      });
      
    } catch (error: any) {
      console.error("Error initializing Conversations client:", error);
    }
  }, [setupConversationEvents]);

  // Update join room to use separate token for Conversations
  const joinRoom = async () => {
    if (!userName.trim()) return setStatus("Vui lòng nhập tên người dùng");
    if (!hasMediaPermission && !(await checkMediaPermissions())) return;

    setIsJoining(true);
    setStatus("Đang kết nối...");

    try {
      // Properly clean up preview track before connecting
      if (previewTrack) {
        previewTrack.detach();
        previewTrack.stop();
        previewStarted.current = false;
      }

      // Get token for video
      const res = await fetch(
        `/api/meeting?identity=${encodeURIComponent(userName)}`
      );
      const { token } = await res.json();
      
      // Get a different token for Conversations from the Auth service
      // For simplicity, map any username to User1, User2, or User3
      // This is just for testing - in production, you would use proper auth
      let conversationsIdentity = "User1"; // Default
      
      // Simple mapping based on first letter of username to distribute among the 3 users
      const firstChar = userName.charAt(0).toLowerCase();
      if (firstChar >= 'a' && firstChar <= 'i') {
        conversationsIdentity = "User1";
      } else if (firstChar >= 'j' && firstChar <= 'r') {
        conversationsIdentity = "User2";
      } else {
        conversationsIdentity = "User3";
      }
      
      console.log(`Using ${conversationsIdentity} for Conversations client`);
      
      try {
        const conversationsToken = await getConversationsToken(conversationsIdentity);
        // Initialize Conversations with the auth service token
        await initializeConversations(conversationsToken);
      } catch (convError) {
        console.error("Error initializing Conversations:", convError);
        // Continue with video even if Conversations fails
      }
      
      // Video connection with original token
      const connectedRoom = await Video.connect(token, {
        name: "MyMeetingRoom",
        audio: true,
        video: { width: 640 },
        // Thêm các tùy chọn để tự động kết nối với mic và camera
        tracks: await Promise.all([
          Video.createLocalAudioTrack(),
          Video.createLocalVideoTrack({width: 640})
        ]),
        // Log cấp độ để gỡ lỗi
        logLevel: 'debug'
      });

      setRoom(connectedRoom);
      setStatus(`Đã kết nối đến phòng: ${connectedRoom.name} [${connectedRoom.participants.size} participants]`);

      // Lấy local video track và lưu lại
      connectedRoom.localParticipant.videoTracks.forEach((publication) => {
        if (publication.track.kind === "video") {
          setLocalVideoTrack(publication.track as LocalVideoTrack);
        }
      });
      
      // Kích hoạt local audio tracks
      connectedRoom.localParticipant.audioTracks.forEach((publication) => {
        if (publication.track) {
          console.log("Enabling local audio track");
          publication.track.enable();
        }
      });
      
      if (connectedRoom.localParticipant.audioTracks.size === 0) {
        console.warn("No local audio tracks found. Audio might not work.");
      } else {
        console.log(`Found ${connectedRoom.localParticipant.audioTracks.size} local audio tracks`);
      }

      // Hiển thị các participant đã ở sẵn trong phòng
      connectedRoom.participants.forEach((participant) => {
        participant.tracks.forEach((publication) => {
          if (publication.isSubscribed && publication.track?.kind === "video") {
            setRemoteParticipants((prev) => [
              ...prev,
              {
                id: participant.sid,
                username: participant.identity,
                track: publication.track as RemoteVideoTrack,
                isMicOn: Array.from(participant.audioTracks.values()).some(
                  (pub) => pub.track?.isEnabled
                ),
              },
            ]);
          }
        });

        participant.on("trackSubscribed", (track) => {
          if (track.kind === "video") {
            setRemoteParticipants((prev) => [
              ...prev,
              {
                id: participant.sid,
                username: participant.identity,
                track: track as RemoteVideoTrack,
                isMicOn: Array.from(participant.audioTracks.values()).some(
                  (pub) => pub.track?.isEnabled
                ),
              },
            ]);
          }
        });
      });

      // Khi participant mới kết nối
      connectedRoom.on("participantConnected", (participant) => {
        console.log(`Participant ${participant.identity} connected`);
        
        // Update status to show participant count
        setStatus(`Đã kết nối đến phòng: ${connectedRoom.name} [${connectedRoom.participants.size + 1} participants]`);
        
        participant.on("trackSubscribed", (track) => {
          if (track.kind === "video") {
            setRemoteParticipants((prev) => [
              ...prev,
              {
                id: participant.sid,
                username: participant.identity,
                track: track as RemoteVideoTrack,
                isMicOn: Array.from(participant.audioTracks.values()).some(
                  (pub) => pub.track?.isEnabled
                ),
              },
            ]);
          }
        });
      });

      // Khi participant rời đi - improved handler for immediate UI updates
      connectedRoom.on("participantDisconnected", (participant) => {
        console.log(`Participant ${participant.identity} disconnected`);
        
        // Immediate state update with functional update pattern
        setRemoteParticipants(prev => 
          prev.filter(p => p.id !== participant.sid)
        );
        
        // Force a re-render after a small delay to ensure layout updates
        setTimeout(() => {
          setStatus(current => `${current.split(' [')[0]} [${connectedRoom.participants.size} participants]`);
        }, 100);
      });

      // Khi ngắt kết nối khỏi phòng
      connectedRoom.on("disconnected", () => {
        setRoom(null);
        setStatus("Đã ngắt kết nối");
        setLocalVideoTrack(null);
        setRemoteParticipants([]);
      });
      
      // Connect to WebSocket for sign language recognition after joining the room
      connectWebSocket();
    } catch (err) {
      setStatus("Lỗi kết nối: " + (err as Error).message);
    } finally {
      setIsJoining(false);
    }
  };

  // Thoát phòng
  const leaveRoom = () => {
    if (room) room.disconnect();
    
    // Also disconnect WebSocket
    if (socket) {
      socket.close();
    }
    
    // Clear any intervals
    if (captureIntervalRef.current) {
      clearInterval(captureIntervalRef.current);
      captureIntervalRef.current = null;
    }
    
    setIsRecognitionActive(false);
    setSocketConnected(false);
    
    // Disconnect Conversations client
    if (conversation) {
      conversation.removeAllListeners();
    }
    
    if (conversationsClient) {
      conversationsClient.removeAllListeners();
      conversationsClient.shutdown();
    }
  };

  // Toggle microphone
  const toggleMic = () => {
    if (room) {
      room.localParticipant.audioTracks.forEach(publication => {
        if (publication.track) {
          if (isMicOn) {
            publication.track.disable();
          } else {
            publication.track.enable();
          }
        }
      });
      setIsMicOn(!isMicOn);
    }
  };

  // Toggle video
  const toggleVideo = () => {
    if (room) {
      room.localParticipant.videoTracks.forEach(publication => {
        if (publication.track) {
          if (isVideoOn) {
            publication.track.disable();
          } else {
            publication.track.enable();
          }
        }
      });
      setIsVideoOn(!isVideoOn);
    }
  };

  // Effect to get access to video element for frame capture
  useEffect(() => {
    if (localVideoTrack && room) {
      // Find the video element associated with the local track
      const videoElements = localVideoTrack.detach();
      
      // Store reference to use for frame capture
      if (videoElements.length > 0) {
        videoElementRef.current = videoElements[0] as HTMLVideoElement;
        
        // Reattach to avoid disrupting the video display
        videoElements.forEach(element => {
          localVideoTrack.attach(element);
        });
      }
    }
  }, [localVideoTrack, room]);

  useEffect(() => {
    checkMediaPermissions();
    return () => {
      if (previewTrack) {
        previewTrack.detach();
        previewTrack.stop();
      }
      room?.disconnect();
      
      // Clean up WebSocket
      if (socket) {
        socket.close();
      }
      
      // Clear any intervals
      if (captureIntervalRef.current) {
        clearInterval(captureIntervalRef.current);
        captureIntervalRef.current = null;
      }
      
      // Clear processing interval
      if (processingIntervalRef.current) {
        clearInterval(processingIntervalRef.current);
        processingIntervalRef.current = null;
      }
    };
  }, [previewTrack, room, checkMediaPermissions, socket]);

  // Clean up old predictions to prevent memory issues
  useEffect(() => {
    const cleanupInterval = setInterval(() => {
      // Keep only the last 10 minutes of predictions to prevent memory issues
      const tenMinutesAgo = Date.now() - 10 * 60 * 1000;
      setPredictionQueue(prevQueue => 
        prevQueue.filter(item => item.timestamp >= tenMinutesAgo)
      );
    }, 60000); // Run cleanup every minute
    
    return () => {
      clearInterval(cleanupInterval);
    };
  }, []);

  // Debug interval for logging prediction queue state
  useEffect(() => {
    const debugInterval = setInterval(() => {
      if (predictionQueue.length > 0 && isRecognitionActive) {
        console.log(`[${new Date().toISOString()}] Queue stats:`, {
          count: predictionQueue.length,
          processedCount: predictionQueue.filter(p => p.processed).length,
          unprocessedCount: predictionQueue.filter(p => !p.processed).length
        });
        // Only log last prediction instead of the last 5
        if (predictionQueue.length > 0) {
          console.log(`Last prediction: "${predictionQueue[predictionQueue.length - 1].prediction}"`);
        }
      }
    }, 30000); // Log every 30 seconds instead of 10
    
    return () => {
      clearInterval(debugInterval);
    };
  }, [predictionQueue, isRecognitionActive]);

  // Add an effect to handle layout changes when participants list changes
  useEffect(() => {
    if (room && remoteParticipants) {
      console.log(`Layout should update: ${remoteParticipants.length + 1} participants total`);
    }
  }, [remoteParticipants, room]);

  const handleNewPrediction = useCallback((prediction: string, confidence: number) => {
    // Only add meaningful predictions to the queue
    if (prediction !== "No sign detected" && confidence > 0) {
      console.log(`Adding new prediction to queue: "${prediction}" (${confidence}%)`);
      
      const newPrediction = {
        prediction,
        confidence,
        timestamp: Date.now(),
        processed: false
      };
      
      // Update ref directly for debugging
      currentPredictionsRef.current = [...currentPredictionsRef.current, newPrediction];
      console.log("Current predictions in ref:", currentPredictionsRef.current.length);
      
      setPredictionQueue(prevQueue => {
        const newQueue = [...prevQueue, newPrediction];
        
        // Update combined subtitles
        const last20Predictions = newQueue
          .slice(-20)
          .sort((a, b) => a.timestamp - b.timestamp)
          .map(item => item.prediction);
        
        // Join predictions into a sentence
        const combinedText = last20Predictions.join(' ');
        setCombinedSubtitles(combinedText);
        
        console.log("Queue updated, new length:", newQueue.length);
        return newQueue;
      });
    }
  }, []);

  // Check and setup audio
  useEffect(() => {
    if (room) {
      console.log("Checking audio setup in room");
      // Log các audio tracks hiện tại
      const audioTracks = Array.from(room.localParticipant.audioTracks.values());
      console.log(`Local audio tracks: ${audioTracks.length}`);
      
      // Kích hoạt tất cả các audio tracks
      audioTracks.forEach(publication => {
        if (publication.track) {
          console.log(`Enabling audio track: ${publication.trackName}`);
          publication.track.enable();
        }
      });
      
      // Nếu không có audio tracks, có thể cần thêm mới
      if (audioTracks.length === 0) {
        console.log("No audio tracks found, attempting to create one");
        navigator.mediaDevices.getUserMedia({ audio: true })
          .then(stream => {
            console.log("Got audio stream, creating track");
            const audioTrack = stream.getAudioTracks()[0];
            if (audioTrack) {
              console.log("Audio track created successfully");
              // Có thể thêm track vào room ở đây nếu cần
            }
          })
          .catch(err => console.error("Error getting audio stream:", err));
      }
    }
  }, [room]);

  // Update the modified speech recognition implementation
  const startSpeechRecognition = useCallback(() => {
    if (!isListening) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      
      if (!SpeechRecognition) {
        console.error("Speech recognition not supported in this browser");
        return;
      }
      
      const recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      
      recognition.onstart = () => {
        console.log("Speech recognition started");
        setIsListening(true);
        
        // Start interval to send text every 10 seconds
        speechIntervalRef.current = setInterval(() => {
          const currentText = speechText.trim();
          
          // Only send if there's new text and it's different from last sent
          if (currentText && currentText !== lastSentSpeechRef.current) {
            console.log("Sending speech text to chat:", currentText);
            sendChatMessage(currentText);
            lastSentSpeechRef.current = currentText;
            setSpeechText("");
          }
        }, 10000);
      };
      
      recognition.onresult = (event: SpeechRecognitionEvent) => {
        const transcript = Array.from(event.results)
          .map(result => result[0].transcript)
          .join('');
        
        setSpeechText(transcript);
      };
      
      recognition.onend = () => {
        console.log("Speech recognition ended");
        setIsListening(false);
        
        // Send any remaining text when speech recognition ends
        const finalText = speechText.trim();
        if (finalText && finalText !== lastSentSpeechRef.current) {
          sendChatMessage(finalText);
        }
        
        // Clear interval
        if (speechIntervalRef.current) {
          clearInterval(speechIntervalRef.current);
          speechIntervalRef.current = null;
        }
        
        // Reset
        lastSentSpeechRef.current = "";
        setSpeechText("");
      };
      
      recognition.onerror = (event) => {
        console.error("Speech recognition error", event);
        setIsListening(false);
        
        // Clear interval on error
        if (speechIntervalRef.current) {
          clearInterval(speechIntervalRef.current);
          speechIntervalRef.current = null;
        }
      };
      
      recognitionRef.current = recognition;
      recognition.start();
    } else {
      stopSpeechRecognition();
    }
  }, [isListening, speechText]);
  
  const stopSpeechRecognition = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      recognitionRef.current = null;
      setIsListening(false);
      
      // Clear interval
      if (speechIntervalRef.current) {
        clearInterval(speechIntervalRef.current);
        speechIntervalRef.current = null;
      }
      
      // Send any remaining text
      const finalText = speechText.trim();
      if (finalText && finalText !== lastSentSpeechRef.current) {
        sendChatMessage(finalText);
        setSpeechText("");
      }
      
      lastSentSpeechRef.current = "";
    }
  }, [speechText]);

  // Update sendChatMessage to use Twilio Conversations
  const sendChatMessage = useCallback((messageText?: string) => {
    const textToSend = messageText || newMessage;
    
    if (!textToSend.trim() || !conversation) return;
    
    // Send via Twilio Conversations
    conversation.sendMessage(textToSend.trim())
      .then(() => {
        console.log("Message sent successfully");
        
        // Only clear input if using the input field (not speech)
        if (!messageText) {
          setNewMessage("");
        }
      })
      .catch((error: any) => {
        console.error("Error sending message:", error);
        
        // Add message locally if sending fails
        const fallbackMessage: ChatMessage = {
          sender: userName,
          content: textToSend,
          timestamp: Date.now(),
          isLocal: true
        };
        
        setChatMessages(prev => [...prev, fallbackMessage]);
      });
    
    // Scroll to bottom
    setTimeout(() => {
      chatEndRef.current?.scrollIntoView({ behavior: "smooth" });
    }, 100);
  }, [newMessage, conversation, userName]);

  // Add cleanup for speech interval
  useEffect(() => {
    return () => {
      if (speechIntervalRef.current) {
        clearInterval(speechIntervalRef.current);
        speechIntervalRef.current = null;
      }
    };
  }, []);

  // Giao diện
  return (
    <div className={cn(
      "flex flex-col h-screen",
      isDarkMode ? "bg-gray-900 text-white" : "bg-white text-gray-800"
    )}>
      {/* Header */}
      <div className={cn(
        "py-3 px-6 flex justify-between items-center shadow-lg border-b",
        isDarkMode ? "bg-gray-800 border-gray-700" : "bg-white border-gray-200"
      )}>
        <div className="flex items-center space-x-2">
          <span className="text-xl font-bold text-yellow-500">SLOM</span>
          <span className={cn("text-opacity-50", isDarkMode ? "text-gray-400" : "text-gray-500")}>|</span>
          <span className="text-lg">{room ? room.name : "Meeting Room"}</span>
        </div>
        <div className={cn(
          "text-sm",
          isDarkMode ? "text-gray-400" : "text-gray-500"
        )}>
          {status}
        </div>
      </div>

      {/* Main content */}
      <div className="flex-1 overflow-hidden flex flex-col p-4">
        {!room ? (
          <div className="flex flex-col items-center justify-center h-full max-w-md mx-auto w-full">
            <div className={cn(
              "p-8 rounded-xl shadow-xl w-full border",
              isDarkMode ? "bg-gray-800 border-gray-700" : "bg-white border-gray-200"
            )}>
              <h2 className="text-2xl font-bold mb-6 text-center">Join Meeting</h2>
              
              {/* Camera preview */}
              <div className={cn(
                "aspect-video mb-4 rounded-lg overflow-hidden relative",
                isDarkMode ? "bg-gray-700" : "bg-gray-100"
              )}>
                <video
                  ref={previewRef}
                  autoPlay
                  playsInline
                  muted
                  className="w-full h-full object-contain"
                />
                
                {!hasMediaPermission && (
                  <div className="absolute inset-0 flex items-center justify-center bg-black/50">
                    <button
                      onClick={startVideoPreview}
                      className="bg-blue-600 hover:bg-blue-700 text-white px-4 py-2 rounded"
                    >
                      Enable Camera
                    </button>
                  </div>
                )}
                
                {hasMediaPermission && (
                  <div className={cn(
                    "absolute bottom-2 left-2 px-2 py-1 text-xs rounded",
                    isDarkMode ? "bg-black/50 text-white" : "bg-white/50 text-black"
                  )}>
                    Preview
                  </div>
                )}
              </div>
              
              <div className="space-y-4">
                <div>
                  <label htmlFor="username" className={cn(
                    "text-sm block mb-1",
                    isDarkMode ? "text-gray-400" : "text-gray-600"
                  )}>
                    Your Name
                  </label>
                  <Input
                    id="username"
                    type="text"
                    value={userName}
                    onChange={(e) => setUserName(e.target.value)}
                    placeholder="Enter your name"
                    className={cn(
                      "w-full",
                      isDarkMode 
                        ? "bg-gray-700 border-gray-600 text-white placeholder-gray-400" 
                        : "bg-white border-gray-300 text-black placeholder-gray-500"
                    )}
                  />
                </div>
                <Button
                  onClick={joinRoom}
                  disabled={!hasMediaPermission || isJoining}
                  className="w-full bg-blue-600 hover:bg-blue-700 text-white"
                >
                  {isJoining ? "Connecting..." : "Join Meeting"}
                </Button>
                
                {!hasMediaPermission && (
                  <p className="text-red-500 text-sm mt-2">
                    Please allow access to camera and microphone
                  </p>
                )}
              </div>
            </div>
          </div>
        ) : (
          <>
            <div className="flex-1 overflow-hidden relative">
              <MeetingLayout numberOfParticipants={remoteParticipants.length + 1}>
                {localVideoTrack && (
                  <UserCard
                    track={localVideoTrack}
                    username={userName || "You"}
                    isMicOn={isMicOn}
                    isDarkMode={isDarkMode}
                  />
                )}
                {remoteParticipants.map((p) => (
                  <UserCard
                    key={p.id}
                    track={p.track}
                    username={p.username}
                    isMicOn={p.isMicOn}
                    isDarkMode={isDarkMode}
                  />
                ))}
              </MeetingLayout>
            </div>

            {/* Sign language subtitles */}
            {room && (
              <div className={cn(
                "w-full mb-4 p-4 rounded-lg text-center",
                isDarkMode ? "bg-gray-800" : "bg-gray-100",
                isRecognitionActive ? "" : "opacity-50"
              )}>
                <div className="flex justify-between items-center mb-2">
                  <span className={cn(
                    "text-sm font-medium",
                    isDarkMode ? "text-gray-400" : "text-gray-600"
                  )}>
                    Sign Language Recognition {socketConnected ? "(Connected)" : "(Disconnected)"}
                  </span>
                  <Button
                    onClick={toggleSignRecognition}
                    variant="outline"
                    className={cn(
                      "text-xs py-1 h-auto",
                      isRecognitionActive 
                        ? "bg-blue-600 hover:bg-blue-700 text-white border-blue-600" 
                        : isDarkMode ? "bg-gray-700" : ""
                    )}
                  >
                    {isRecognitionActive ? "Disable Subtitles" : "Enable Subtitles"}
                  </Button>
                </div>
                
                {/* Enhanced subtitles display showing both processed and raw detection */}
                <div className="space-y-2">
                  {/* Combined subtitles from all predictions */}
                  <div className={cn(
                    "py-3 px-4 rounded-lg text-xl font-medium min-h-[60px] flex items-center justify-center",
                    isDarkMode ? "bg-gray-700" : "bg-white"
                  )}>
                    {combinedSubtitles || "Waiting for sign language detection..."}
                  </div>
                  
                  {/* Processed LLM subtitles */}
                  {processedSubtitles && (
                    <div className={cn(
                      "py-3 px-4 rounded-lg text-lg font-medium min-h-[40px] flex items-center justify-center",
                      isDarkMode ? "bg-gray-700/70" : "bg-white/80",
                      "text-sm opacity-80"
                    )}>
                      <span className="mr-2 text-xs uppercase tracking-wider font-bold opacity-60">AI Processed:</span>
                      {processedSubtitles}
                    </div>
                  )}
                  
                  {/* Current detection */}
                  <div className={cn(
                    "py-2 px-4 rounded-lg text-sm min-h-[30px] flex items-center justify-center",
                    isDarkMode ? "bg-gray-700/50" : "bg-white/70",
                    signPrediction === "No sign detected" ? "text-gray-500 italic" : "",
                    "opacity-60"
                  )}>
                    <span className="mr-2 text-xs uppercase tracking-wider font-bold">Current:</span>
                    {signPrediction}
                    {signConfidence > 0 && signPrediction !== "No sign detected" && (
                      <span className={cn(
                        "ml-2 text-xs py-0.5 px-1.5 rounded",
                        isDarkMode ? "bg-gray-600 text-gray-300" : "bg-gray-200 text-gray-700"
                      )}>
                        {signConfidence}%
                      </span>
                    )}
                  </div>
                </div>
              </div>
            )}

            {/* Meeting controls */}
            <div className={cn(
              "mt-4 flex justify-center py-4 rounded-lg",
              isDarkMode ? "bg-gray-800" : "bg-gray-100"
            )}>
              <div className="flex items-center space-x-4">
                <Button
                  onClick={toggleMic}
                  variant="outline"
                  className={cn(
                    "rounded-full p-3 h-12 w-12",
                    isMicOn 
                      ? (isDarkMode ? "bg-gray-700" : "bg-white border-gray-300") 
                      : "bg-red-600 hover:bg-red-700 border-red-600"
                  )}
                >
                  {isMicOn ? <MicrophoneIcon size={20} /> : <MicrophoneOffIcon size={20} />}
                </Button>
                
                <Button
                  onClick={toggleVideo}
                  variant="outline"
                  className={cn(
                    "rounded-full p-3 h-12 w-12",
                    isVideoOn 
                      ? (isDarkMode ? "bg-gray-700" : "bg-white border-gray-300") 
                      : "bg-red-600 hover:bg-red-700 border-red-600"
                  )}
                >
                  {isVideoOn ? <VideoIcon size={20} /> : <VideoOffIcon size={20} />}
                </Button>
                
                <Button
                  onClick={leaveRoom}
                  className="bg-red-600 hover:bg-red-700 rounded-full p-3 h-12 w-12"
                >
                  <PhoneIcon size={20} />
                </Button>
              </div>
            </div>
          </>
        )}
      </div>
      
      {/* Participants info panel */}
      {room && remoteParticipants.length > 0 && (
        <div className={cn(
          "hidden md:block absolute right-0 top-16 p-4 rounded-l-lg shadow-lg border-l border-t border-b",
          isDarkMode 
            ? "bg-gray-800 border-gray-700 text-gray-300" 
            : "bg-white border-gray-200 text-gray-700"
        )}>
          <h3 className="text-lg font-semibold mb-2">Participants ({remoteParticipants.length + 1})</h3>
          <ul className="space-y-2">
            <li className="flex items-center space-x-2">
              <div className="w-2 h-2 rounded-full bg-green-500"></div>
              <span>{userName || "You"} (You)</span>
            </li>
            {remoteParticipants.map(p => (
              <li key={p.id} className="flex items-center space-x-2">
                <div className="w-2 h-2 rounded-full bg-green-500"></div>
                <span>{p.username}</span>
              </li>
            ))}
          </ul>
        </div>
      )}

      {/* Chat sidebar */}
      {room && (
        <div className={cn(
          "fixed right-0 top-16 bottom-0 z-10 transition-all duration-300 flex flex-col",
          isChatOpen 
            ? "w-80 opacity-100 translate-x-0" 
            : "w-0 opacity-0 translate-x-full",
          isDarkMode ? "bg-gray-800 text-white" : "bg-white text-gray-900",
          "shadow-lg border-l"
        )}>
          <div className="p-4 border-b flex justify-between items-center">
            <h3 className="font-semibold">Chat</h3>
            <Button 
              variant="ghost" 
              size="sm" 
              className="h-8 w-8 p-0 rounded-full"
              onClick={() => setIsChatOpen(false)}
            >
              <ArrowRight size={16} />
            </Button>
          </div>
          
          {/* Messages area */}
          <div className="flex-1 overflow-y-auto p-4 space-y-4">
            {chatMessages.map((msg, idx) => (
              <div 
                key={idx} 
                className={cn(
                  "max-w-[80%] p-3 rounded-lg",
                  msg.isLocal 
                    ? "ml-auto bg-blue-600 text-white rounded-br-none" 
                    : "mr-auto bg-gray-200 text-gray-900 dark:bg-gray-700 dark:text-white rounded-bl-none"
                )}
              >
                <div className="text-xs opacity-70 mb-1">{msg.sender}</div>
                <div>{msg.content}</div>
              </div>
            ))}
            <div ref={chatEndRef} />
          </div>
          
          {/* Input area */}
          <div className="p-4 border-t flex gap-2">
            <Input
              value={newMessage}
              onChange={(e) => setNewMessage(e.target.value)}
              placeholder="Type a message..."
              className={isDarkMode ? "bg-gray-700 border-gray-600" : ""}
              onKeyDown={(e) => e.key === 'Enter' && sendChatMessage()}
            />
            <Button 
              onClick={() => sendChatMessage()}
              size="icon"
              className="h-10 w-10 rounded-full"
            >
              <Send size={18} />
            </Button>
          </div>
        </div>
      )}
      
      {/* Chat toggle button */}
      {room && (
        <Button
          onClick={() => setIsChatOpen(true)}
          className={cn(
            "fixed right-4 bottom-20 rounded-full p-3 h-12 w-12 z-10",
            isChatOpen ? "hidden" : "flex",
            isDarkMode ? "bg-gray-700" : "bg-white",
            isDarkMode ? "hover:bg-gray-600" : "hover:bg-gray-100"
          )}
          variant="outline"
        >
          <MessageSquare size={20} />
        </Button>
      )}
      
      {/* Speech-to-text toggle button */}
      {room && (
        <Button
          onClick={startSpeechRecognition}
          className={cn(
            "fixed right-4 bottom-36 rounded-full p-3 h-12 w-12 z-10",
            isDarkMode ? "bg-gray-700" : "bg-white",
            isListening ? "bg-green-500 hover:bg-green-600 text-white" : "",
            isDarkMode && !isListening ? "hover:bg-gray-600" : !isListening ? "hover:bg-gray-100" : ""
          )}
          variant="outline"
        >
          <MicrophoneIcon size={20} />
        </Button>
      )}
      
      {/* Speech-to-text result display */}
      {isListening && speechText && (
        <div className={cn(
          "fixed left-4 right-4 bottom-20 p-4 rounded-lg shadow-lg z-10",
          isDarkMode ? "bg-gray-800" : "bg-white"
        )}>
          <div className="flex justify-between items-center mb-2">
            <div className="flex items-center">
              <div className="h-2 w-2 rounded-full bg-green-500 mr-2 animate-pulse"></div>
              <span className="text-sm font-medium">Listening...</span>
            </div>
            <Button 
              variant="ghost" 
              size="sm" 
              className="h-6 w-6 p-0 rounded-full"
              onClick={stopSpeechRecognition}
            >
              <MicrophoneOffIcon size={14} />
            </Button>
          </div>
          <p className={cn(
            "text-sm",
            isDarkMode ? "text-gray-300" : "text-gray-700"
          )}>
            {speechText}
          </p>
        </div>
      )}
    </div>
  );
}
